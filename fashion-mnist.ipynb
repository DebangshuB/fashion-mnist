{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST Dataset Classification using Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset was downloaded from [Kaggle](https://www.kaggle.com/zalando-research/fashionmnist).\n",
    "* The dataset was downloaded and stored locally.\n",
    "* The dataset available was already flattened and stored as arrays in a .csv file separated as train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 : Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 : Reading the training set and splitting into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('fashion-mnist_train.csv')\n",
    "y = dataset_train.iloc[:, 0].values\n",
    "x = dataset_train.iloc[:, 1:-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 : Reading the test set and splitting into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv('fashion-mnist_test.csv')\n",
    "y_test = dataset_test.iloc[:, 0].values\n",
    "x_test = dataset_test.iloc[:, 1:-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 : Splitting the training set into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 : Standardizing the training , test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_val = sc.transform(X_val)\n",
    "X_test = sc.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 : One Hot Encoding the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "Y_train = LabelBinarizer().fit_transform(Y_train)\n",
    "Y_val = LabelBinarizer().fit_transform(Y_val)\n",
    "Y_test = LabelBinarizer().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 : Creating the model and fitting it to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.4800 - accuracy: 0.8279 - val_loss: 0.3735 - val_accuracy: 0.8620\n",
      "Epoch 2/12\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.3439 - accuracy: 0.8744 - val_loss: 0.3596 - val_accuracy: 0.8686\n",
      "Epoch 3/12\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.3025 - accuracy: 0.8898 - val_loss: 0.3349 - val_accuracy: 0.8809\n",
      "Epoch 4/12\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.2731 - accuracy: 0.8989 - val_loss: 0.3162 - val_accuracy: 0.8861\n",
      "Epoch 5/12\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.2462 - accuracy: 0.9076 - val_loss: 0.3301 - val_accuracy: 0.8823\n",
      "Epoch 6/12\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.2325 - accuracy: 0.9130 - val_loss: 0.3203 - val_accuracy: 0.8842\n",
      "Epoch 7/12\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.2186 - accuracy: 0.9188 - val_loss: 0.3266 - val_accuracy: 0.8865\n",
      "Epoch 8/12\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.1973 - accuracy: 0.9263 - val_loss: 0.3358 - val_accuracy: 0.8906\n",
      "Epoch 9/12\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.1814 - accuracy: 0.9322 - val_loss: 0.3464 - val_accuracy: 0.8864\n",
      "Epoch 10/12\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1718 - accuracy: 0.9372 - val_loss: 0.3522 - val_accuracy: 0.8896\n",
      "Epoch 11/12\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1572 - accuracy: 0.9423 - val_loss: 0.3483 - val_accuracy: 0.8918\n",
      "Epoch 12/12\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.1426 - accuracy: 0.9476 - val_loss: 0.3586 - val_accuracy: 0.8949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x156b80f12b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = tf.keras.models.Sequential() \n",
    "ann.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "ann.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "ann.fit(X_train, Y_train, batch_size = 128, epochs = 12,validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Evalutating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3638 - accuracy: 0.8950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.363785058259964, 0.8949999809265137]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.evaluate(X_test,Y_test,verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DS]",
   "language": "python",
   "name": "conda-env-DS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
